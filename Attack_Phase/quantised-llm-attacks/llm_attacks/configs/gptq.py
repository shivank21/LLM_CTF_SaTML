import os
import torch
from auto_gptq import BaseQuantizeConfig

os.sys.path.append("..")
from configs.template import get_config as default_config

def get_config():
    
    config = default_config()
    
    config.tokenizer_paths=['TheBloke/vicuna-7B-v1.5-GPTQ']
    config.model_paths=['TheBloke/vicuna-7B-v1.5-GPTQ']
    config.batch_size = 128
    quantize_config = BaseQuantizeConfig(
        bits=4,
        group_size=128,
        desc_act=False,
        model_name_or_path="TheBloke/vicuna-7B-v1.5-GPTQ",
        model_file_base_name="model"
    )
    config.model_kwargs=[{
        "use_safetensors":True,
        "trust_remote_code":False,
        "device":"cuda:0",
        "use_triton":False,
        "quantize_config":quantize_config
    }]
    